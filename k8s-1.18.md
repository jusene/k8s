## Kubernetes 1.18.2 高可用集群安装

|  IP   | Hostname | OS |CPU | Memory | role |
| ----- | ----- | ----- | ----- | ----- | ----- |
| 192.168.88.38 | k8s-master1 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.39 | k8s-master2 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.40 | k8s-master3 | Centos7.5 | 4 | 8G | etcd， master |
| 192.168.88.41 | k8s-master4 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.42 | k8s-master5 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.43 | k8s-node1 | Centos7.5 | 8 | 16G | node |
| 192.168.88.44 | k8s-node1 | Centos7.5 | 8 | 16G | node |
| 192.168.88.31 | HA1 | Centos7.5 | 2 | 4G | haproxy，keepalived |
| 192.168.88.32 | HA2 | Centos7.5 | 2 | 4G | haproxy，keepalived |


VIP地址 192.168.88.30 master节点的keepalived+haproxy来选择VIP归属保持高可用

### 准备工作

- 时间同步
- 双机互信
- 关闭selinux
```
setenforce 0
sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config
```
- 关闭防火墙
```
systemctl stop firwalld
systemctl disable firewalld
systemctl mask firewalld
```
- 关闭swap
```
swapoff -a && sysctl -w vm.swappiness=0
sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab
```
- 升级系统
```
yum update
```
- 安装通用包
```
yum install -y epel-release wget git jq psmisc socat ipvsadm ipset sysstat libseccomp curl net-tools ethtool
```
- 支持ipvs
```
module=(
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
br_netfilter
  )
for kernel_module in ${module[@]};do
    /sbin/modinfo -F filename $kernel_module |& grep -qv ERROR && echo $kernel_module >> /etc/modules-load.d/ipvs.conf || :
done
systemctl enable --now systemd-modules-load.service
```
- 内核参数
```
# net
## 开启time_wait状态重用机制
net.ipv4.tcp_tw_reuse = 1
## 减少允许time_wait状态的数量，默认180000
net.ipv4.tcp_max_tw_buckets = 6000
## 减少fin_wait_2状态的时间，默认60，防止对端长时间不响应导致占用大量的socket套接字
net.ipv4.tcp_fin_timeout = 10
## 在放弃连接之前syn重试的次数
net.ipv4.tcp_syn_retries = 1
## 定义了内核在放弃连接之前所送出的syn+ack的数据，默认5，大约会花费180秒
net.ipv4.tcp_synack_retries = 1
## 防治ddos攻击，synflood
net.ipv4.tcp_syncookies = 1
## 系统最多有多少个套接字不被关联到任一个用户句柄，所谓的孤儿连接，简单防护ddos工具，内存增大这个值也应该被增大
net.ipv4.tcp_max_orphans = 3276800
## 半连接队列，对于未获得对方确认的连接请求，可以保存在这个队列，服务器网络异常中断可以排查这个参数
net.ipv4.tcp_max_syn_backlog = 262144
## 每个端口接受的数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目
net.core.netdev_max_backlog = 262144
## 表示每个套接字所允许的最大缓冲区的大小
net.core.optmem_max = 81920 
## 关闭tcp时间戳功能，tcp存在一种行为，可以缓存每个连接最新的时间戳，后续请求中如果时间戳小于缓存的时间戳，即视为无效，相应的数据包会被丢弃
net.ipv4.tcp_timestamps = 0
## 间隔多久发一次keepalive探测包，默认7200
net.ipv4.tcp_keepalive_time = 30
## 探测失败后，间隔多久后重新探测，默认75秒
net.ipv4.tcp_keepalive_intvl = 30
## 探测失败后，最多尝试几次，默认9次
net.ipv4.tcp_keepalive_probes = 3
## 根据业务尽量避免业务端口被随机使用
net.ipv4.ip_local_port_range = 12000 65000
## 默认的TCP数据接收窗口大小
net.core.rmem_default = 8388608
## 最大的TCP数据接收窗口大小
net.core.rmem_max = 16777216
## 默认发送TCP数据窗口大小
net.core.wmem_default = 8388608
## 最大的TCP发送数据窗口大小
net.core.wmem_max = 16777216
## 内存使用的下限 警戒值 上限值（内存页）
net.ipv4.tcp_mem = 94500000 915000000 927000000
## socket接收缓冲区内存使用的下限 警戒值 上限（内存页）
net.ipv4.tcp_rmem = 4096  87380   4194304
## socket发送缓冲区内存使用的下限 警戒值 上限（内存页）
net.ipv4.tcp_wmem = 4096  16384   4194304
## 启用有选择的应答,通过有选择地应答乱序接收到的报文来提高性能
net.ipv4.tcp_sack = 1
## 启用转发应答，可以进行有选择应答（SACK）从而减少拥塞情况的发生
net.ipv4.tcp_fack = 1
## 启用RFC 1323定义的window scaling，要支持超过64KB的TCP窗口，必须启用该值
net.ipv4.tcp_window_scaling = 1
## 开启反向路径过滤
net.ipv4.conf.default.rp_filter = 1
## 禁用ip源路由
net.ipv4.conf.default.accept_source_route = 0
## 开启路由转发功能
net.ipv4.ip_forward = 1
## 关闭ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

## iptables不对bridge的数据进行处理
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
# kernel
## 以字节为单位规定单一信息队列的最大值
kernel.msgmnb = 65536
## 以字节为单位规定信息队列中任意信息的最大允许的大小
kernel.msgmax = 65536
## 以字节为单位规定一次在该系统中可以使用的共享内存总量
kernel.shmall = 4294967296
## 以字节为单位内核可允许的最大共享内存 
kernel.shmmax = 68719476736
## 使用sysrq组合键是了解系统目前运行情况，为安全起见设为0关闭
kernel.sysrq = 0
## 控制core文件的文件名是否添加pid作为扩展
kernel.core_uses_pid = 1

# mem
## 优先互动性并尽量避免将进程装换出物理内存
vm.swappiness = 0
## 定义一个进程能够拥有的最多的内存区域，jvm要求高时，这个值也必须调大
vm.max_map_count = 65535
```

- 检查系统内核和模块是否适合运行docker
```
curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh > check-config.sh
bash ./check-config.sh
....
- CONFIG_USER_NS: enabled
  (RHEL7/CentOS7: User namespaces disabled; add 'user_namespace.enable=1' to boot command line)
....
```
解决:
```
# kernel 设置
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
# 写入配置文件
echo "user.max_user_namespaces=10000" >> /etc/sysctl.conf
# 重启
reboot
```
参考：https://www.123si.org/os/article/centos-7-enable-user-namespaces/

- 准备Kubernetes二进制包

```
docker pull jusene/k8s-1.18.2-bin
docker run -d --rm jusene/k8s-1.18.2-bin:latest sleep 300
docker cp 1ee:/kubernetes-1.18.2.tar.gz .
tar xf kubernetes-1.18.2.tar.gz -C /usr/local/bin
# 分发到全部的master节点 kube-apiserver kube-scheduler kube-controller-manager kubectl kubelet kube-proxy
# 分发到全部的node节点 kubelet kube-proxy
```

- 准备Kubernetes CNI二进制文件

```
wget https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz
tar xf cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin

## CNI分发到其他节点的 /opt/cni/bin 下
```

### 安装ETCD集群

- CN: Common Name, apiserver 会从证书中提取该字段作为请求的用户名 (User Name)
- O Organization, apiserver 会从证书中提取该字段作为请求用户所属的组 (Group)

准备openssl.cnf
```
[ req ]
default_bits = 2048
default_md = sha256
distinguished_name = req_distinguished_name

[req_distinguished_name]

[ v3_ca ]
basicConstraints = critical, CA:TRUE
keyUsage = critical, digitalSignature, keyEncipherment, keyCertSign

[ v3_req_server ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth

[ v3_req_client ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth

[ v3_req_apiserver ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names_cluster

[ v3_req_etcd ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
subjectAltName = @alt_names_etcd

[ alt_names_cluster ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
DNS.6 = localhost
IP.1 = 10.50.0.1  # ClusterServiceIP地址
IP.2 = 127.0.0.1
IP.3 = 192.168.88.30  # VIP地址
IP.4 = 10.50.0.200  # kube DNS地址

[ alt_names_etcd ]
DNS.1 = localhost
IP.1 = 127.0.0.1
IP.2 = 192.168.88.38
IP.3 = 192.168.88.39
IP.4 = 192.168.88.40
IP.5 = 192.168.88.41
IP.6 = 192.168.88.42
```

#### 生成证书

- ca证书
```
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj "/CN=etcd-ca" -extensions v3_ca -out ca.crt -days 36500
```

- etcd证书
```
openssl genrsa -out etcd.key 2048
openssl req -new -key etcd.key -subj "/CN=etcd/O=system:masters" -out etcd.csr
openssl x509 -in etcd.csr -req -CA ca.crt -CAkey ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd.crt -days 36500
```

- 准备证书
```
mkdir /etc/etcd/etcdSSL
cp ca.crt etcd.key etcd.pem /etc/etcd/etcdSSL/
chown -R etcd /etc/etcd/etcdSSL
# 同样分发到全部etcd节点
```

- 安装etcd
```
yum install -y etcd
```

- 准备配置文件
```
vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/usr/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/etcd/etcdSSL/etcd.crt \
  --key-file=/etc/etcd/etcdSSL/etcd.key \
  --peer-cert-file=/etc/etcd/etcdSSL/etcd.crt \
  --peer-key-file=/etc/etcd/etcdSSL/etcd.key \
  --trusted-ca-file=/etc/etcd/etcdSSL/ca.crt \
  --peer-trusted-ca-file=/etc/etcd/etcdSSL/ca.crt \
  --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
  --listen-peer-urls=${ETCD_LISTEN_PEER_URLS} \
  --listen-client-urls=${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
  --advertise-client-urls=${ETCD_ADVERTISE_CLIENT_URLS} \
  --initial-cluster-token=${ETCD_INITIAL_CLUSTER_TOKEN} \
  --initial-cluster=${ETCD_CLUSTER}\
  --initial-cluster-state new \
  --data-dir=${ETCD_DATA_DIR}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

- etcd.conf
```
ETCD_NAME=etcd0
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.88.38:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.88.38:2379"

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.88.38:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.88.38:2379"
ETCD_CLUSTER="etcd0=https://192.168.88.38:2380,etcd1=https://192.168.88.39:2380,etcd2=https://192.168.88.40:2380,etcd3=https://192.168.88.41:2380,etcd4=https://192.168.88.42:2380"

# 其他节点安装配置相应修改
```

5个etcd节点，需要同时启动3个节点整个集群才可以使用

```
etcdctl --cert-file=/etc/etcd/etcdSSL/etcd.crt --key-file=/etc/etcd/etcdSSL/etcd.key --ca-file=/etc/etcd/etcdSSL/ca.crt cluster-health
member 3f606110d03e3132 is healthy: got healthy result from https://192.168.88.39:2379
member 3f8913ef4acb01e6 is healthy: got healthy result from https://192.168.88.41:2379
member 72cbf487f90ef57b is healthy: got healthy result from https://192.168.88.42:2379
member d58a14e5945489fa is healthy: got healthy result from https://192.168.88.38:2379
member dd01c3c17c734d25 is healthy: got healthy result from https://192.168.88.40:2379
cluster is healthy
```

### 安装Kubernetes Master

#### 部署HA(haproxy+keepalived)

```
yum install -y haproxy keepalived
```

- haproxy
```
vim /etc/haproxy/haproxy.cfg

global
  maxconn  5000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

listen stats
  bind    *:8006
  mode    http
  stats   enable
  stats   hide-version
  stats   uri       /stats
  stats   refresh   30s
  stats   realm     Haproxy\ Statistics
  stats   auth      admin:admin

frontend k8s-api
  bind 0.0.0.0:6443
  bind 127.0.0.1:6443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-api

backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 500 maxqueue 256 weight 100
  server api1 192.168.88.38:6443 check
  server api2 192.168.88.39:6443 check
  server api3 192.168.88.40:6443 check
  server api4 192.168.88.41:6443 check
  server api5 192.168.88.42:6443 check
```

- keepalived

```
global_defs {
   vrrp_mcast_group4 224.0.100.20
}


vrrp_script chk_haproxy {
      script "kill -0 `pidof haproxy`"
      interval 2
      weight -10
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens3
    virtual_router_id 100
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass dd@2019
    }
    virtual_ipaddress {
        192.168.88.30/24 dev ens3 label ens3:0
    }
    track_script {
        chk_haproxy
    }
}
```

在两台ha服务器上进行相同的配置

```
systemctl enable haproxy --now
systemctl enable keepalived --now
```

#### 准备证书

- ca
```
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj "/CN=kubernetes-ca" -extensions v3_ca -out ca.crt -days 10000
```

- kube-apiserver
```
openssl genrsa -out apiserver.key 2048
openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -config openssl.cnf -out apiserver.csr
openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req_apiserver -extfile openssl.cnf -out apiserver.crt
```

- apiserver-kubelet-client
```
openssl genrsa -out  apiserver-kubelet-client.key 2048
openssl req -new -key apiserver-kubelet-client.key -subj "/CN=apiserver-kubelet-client/O=system:masters" -out apiserver-kubelet-client.csr
openssl x509 -req -in apiserver-kubelet-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req_client -extfile openssl.cnf -out apiserver-kubelet-client.crt
```

- front-kubelet-client
```
openssl genrsa -out  front-proxy-client.key 2048
openssl req -new -key front-proxy-client.key -subj "/CN=front-proxy-client" -out front-proxy-client.csr
openssl x509 -req -in front-proxy-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req_client -extfile openssl.cnf -out front-proxy-client.crt
```

- kube-scheduler
```
openssl genrsa -out  kube-scheduler.key 2048
openssl req -new -key kube-scheduler.key -subj "/CN=system:kube-scheduler" -out kube-scheduler.csr
openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req_client -extfile openssl.cnf -out kube-scheduler.crt
```

- sa.pub sa.key
```
openssl genrsa -out  sa.key 2048
openssl ecparam -name secp521r1 -genkey -noout -out sa.key
openssl ec -in sa.key -outform PEM -pubout -out sa.pub
openssl req -new -sha256 -key sa.key -subj "/CN=system:kube-controller-manager" -out sa.csr
openssl x509 -req -in sa.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req_client -extfile openssl.cnf -out sa.crt
```

- admin
```
openssl genrsa -out  admin.key 2048
openssl req -new -key admin.key -subj "/CN=kubernetes-admin/O=system:masters" -out admin.csr
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req_client -extfile openssl.cnf -out admin.crt
```

```
mkdir -p /etc/kubernetes/kubernetesTLS
cp admin.crt  apiserver.crt  apiserver-kubelet-client.crt  ca.crt  front-proxy-client.crt  kube-scheduler.crt  sa.crt  sa.pub
admin.key  apiserver.key  apiserver-kubelet-client.key  ca.key  front-proxy-client.key  kube-scheduler.key  sa.key /etc/kubernetes/kubernetesTLS
```

#### 利用证书生成组件kubeconfig

- `--certificate-authority`: 验证根证书
- `--client-certificatr、--client-key`：生成的组件证书和私钥，连接kube-apiserver时会用到
- `--embed-certs=true`: 将ca.crt和组件.crt证书内容嵌入到生成的kubeconfig文件中（不加时，写入的是文件路径）


- kube-controller-manager
```
# 设置集群参数
kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/kubernetesTLS/ca.crt \
    --embed-certs=true \
    --server=https://192.168.88.30:6443 \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=/etc/kubernetes/kubernetesTLS/sa.crt \
    --client-key=/etc/kubernetes/kubernetesTLS/sa.key \
    --embed-certs=true \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 设置上下文参数
kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 设置当前使用的上下文
kubectl config use-context system:kube-controller-manager@kubernetes \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig

# 查看生成的配置文件
kubectl config view --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
```

- kube-scheduler
```
# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/kubernetesTLS/ca.crt --embed-certs=true --server=https://192.168.88.30:6443 --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-scheduler --client-certificate=/etc/kubernetes/kubernetesTLS/kube-scheduler.crt --client-key=/etc/kubernetes/kubernetesTLS/kube-scheduler.key --embed-certs=true --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

# 设置上下文参数
kubectl config set-context system:kube-scheduler@kubernetes --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

# 设置当前使用的上下文
kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

# 查看当前使用的上下文
kubectl config view --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
```

- admin
```
# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/kubernetesTLS/ca.crt --embed-certs=true --server=https://192.168.88.30:6443 --kubeconfig=/etc/kubernetes/admin.kubeconfig

# 设置客户端认证的参数
kubectl config set-credentials kubernetes-admin --client-certificate=/etc/kubernetes/kubernetesTLS/admin.crt --client-key=/etc/kubernetes/kubernetesTLS/admin.key --embed-certs=true --kubeconfig=/etc/kubernetes/admin.kubeconfig 

# 设置上下文参数
kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --kubeconfig=/etc/kubernetes/admin.kubeconfig

# 设置当前使用的上下文
kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=/etc/kubernetes/admin.kubeconfig

# 查看生成的配置文件
kubectl config view --kubeconfig=/etc/kubernetes/admin.kubeconfig
```

分发证书
```
scp -r /etc/kubernetes <other master>:/etc/kubernetes
```

#### kube-apiserver

- /etc/kubernetes/config
````
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
# 表示错误日志记录到文件还是输出到stderr。
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
# 日志等级。设置0则是debug等级
KUBE_LOG_LEVEL="--v=2"

# Should this cluster be allowed to run privileged docker containers
# 允许运行特权容器。
KUBE_ALLOW_PRIV="--allow-privileged=true"

# How the controller-manager, scheduler, and proxy find the apiserver
# 设置master服务器的访问
KUBE_MASTER="--master=https://192.168.88.30:6443"
````

- /etc/kubernetes/apiserver
```
###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
KUBE_API_ADDRESS="--advertise-address=192.168.88.38 --bind-address=192.168.88.38"
#
## The port on the local server to listen on.
KUBE_API_PORT="--insecure-port=0 --secure-port=6443"
#
## Port minions listen on
#KUBELET_PORT="--kubelet-port=10250"
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd-servers=https://192.168.88.38:2379,https://192.168.88.39:2379,https://192.168.88.40:2379,https://192.168.88.41:2379,https://192.168.88.42:2379"
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.50.0.0/16"
#
## default admission control policies
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeClaimResize,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,Priority,PodPreset"

## Add your own!
KUBE_API_ARGS="--authorization-mode=Node,RBAC  \
    --runtime-config=settings.k8s.io/v1alpha1=true  \
    --kubelet-https=true  \
    --enable-bootstrap-token-auth \
    --service-node-port-range=30000-32767 \
    --service-account-key-file=/etc/kubernetes/kubernetesTLS/sa.pub  \
    --tls-cert-file=/etc/kubernetes/kubernetesTLS/apiserver.crt  \
    --tls-private-key-file=/etc/kubernetes/kubernetesTLS/apiserver.key  \
    --client-ca-file=/etc/kubernetes/kubernetesTLS/ca.crt \
    --kubelet-https \
    --service-account-key-file=/etc/kubernetes/kubernetesTLS/ca.key  \
    --storage-backend=etcd3  \
    --etcd-cafile=/etc/etcd/etcdSSL/ca.crt  \
    --etcd-certfile=/etc/etcd/etcdSSL/etcd.crt  \
    --etcd-keyfile=/etc/etcd/etcdSSL/etcd.key  \
    --enable-swagger-ui=true  \
    --apiserver-count=5  \
    --audit-log-maxage=30  \
    --audit-log-maxbackup=3  \
    --audit-log-maxsize=100  \
    --audit-log-path=/var/log/audit.log  \
    --event-ttl=1h \
    --kubelet-client-certificate=/etc/kubernetes/kubernetesTLS/apiserver-kubelet-client.crt \
    --kubelet-client-key=/etc/kubernetes/kubernetesTLS/apiserver-kubelet-client.key \
    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
    --requestheader-client-ca-file=/etc/kubernetes/kubernetesTLS/ca.crt \
    --requestheader-username-headers=X-Remote-User \
    --requestheader-group-headers=X-Remote-Group \
    --requestheader-allowed-names=front-proxy-client \
    --requestheader-extra-headers-prefix=X-Remote-Extra- \
    --proxy-client-cert-file=/etc/kubernetes/kubernetesTLS/front-proxy-client.crt \
    --proxy-client-key-file=/etc/kubernetes/kubernetesTLS/front-proxy-client.key \
    --feature-gates=PodShareProcessNamespace=true"
```

- kube-apiserver.service
````
[Unit]
Description=Kube-apiserver Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

After=network.target
[Service]
Type=notify
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_ETCD_SERVERS \
        $KUBE_API_ADDRESS \
        $KUBE_API_PORT \
        $KUBELET_PORT \
        $KUBE_ALLOW_PRIV \
        $KUBE_SERVICE_ADDRESSES \
        $KUBE_ADMISSION_CONTROL \
        $KUBE_API_ARGS
Restart=always
LimitNOFILE=65536

[Install]
WantedBy=default.target
````

#### kube-scheduler

- /etc/kubernetes/scheduler
```
#wing values are used to configure the kubernetes scheduler

# defaults from config and scheduler should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS="--leader-elect=true \
    --address=127.0.0.1 \
    --kubeconfig=/etc/kubernetes/scheduler.kubeconfig"
```

- kube-scheduler.service
```
[Unit]
Description=Kube-scheduler Service
After=network.target

[Service]
Type=simple
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS

Restart=always
LimitNOFILE=65536

[Install]
WantedBy=default.target
```

#### kube-controller-manager

- /etc/kubernetes/controller-manager
```
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=" --bind-address=127.0.0.1  \
    --allocate-node-cidrs=true \
    --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
    --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
    --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
    --client-ca-file=/etc/kubernetes/kubernetesTLS/ca.crt \
    --cluster-signing-cert-file=/etc/kubernetes/kubernetesTLS/ca.crt \
    --cluster-signing-key-file=/etc/kubernetes/kubernetesTLS/ca.key \
    --service-cluster-ip-range=11.50.0.0/16 \
    --requestheader-client-ca-file=/etc/kubernetes/kubernetesTLS/ca.crt  \
    --service-account-private-key-file=/etc/kubernetes/kubernetesTLS/sa.key \
    --root-ca-file=/etc/kubernetes/kubernetesTLS/ca.crt  \
    --use-service-account-credentials=true \
    --controllers=*,bootstrapsigner,tokencleaner \
    --experimental-cluster-signing-duration=86700h  \
    --feature-gates=RotateKubeletClientCertificate=true \
    --cluster-name=kubernetes  \
    --leader-elect=true  \
    --cluster-cidr=10.16.0.0/16"
```

- kube-controller-manager.service
```
[Unit]
Description=Kube-controller-manager Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=kube-apiserver.service
Requires=kube-apiserver.service
[Service]
Type=simple
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
        $KUBE_LOGTOSTDERR \
        $KUBE_LOG_LEVEL \
        $KUBE_MASTER \
        $KUBE_CONTROLLER_MANAGER_ARGS
Restart=always
LimitNOFILE=65536

[Install]
WantedBy=default.target
```

```
systemctl enable kube-apiserver --now
systemctl enable kube-scheduler --now
systemctl enable kube-controller-manager --now

cp /etc/kubernetes/admin.kubeconifg ~/.kube/conf
```

```
kubectl get cs
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-3               Healthy   {"health":"true"}   
etcd-1               Healthy   {"health":"true"}   
etcd-4               Healthy   {"health":"true"}   
etcd-2               Healthy   {"health":"true"}   
etcd-0               Healthy   {"health":"true"} 

kubectl cluster-info
Kubernetes master is running at https://192.168.88.30:6443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

相同配置在全部master节点上启动master服务

#### 配置bootstrap

只需要在一台master上操作即可，secret会存储在etcd集群中

```
TOKEN_PUB=$(openssl rand -hex 3)
TOKEN_SECRET=$(openssl rand -hex 8)
BOOTSTRAP_TOKEN="${TOKEN_PUB}.${TOKEN_SECRET}"

kubectl -n kube-system create secret generic bootstrap-token-${TOKEN_PUB} \
        --type 'bootstrap.kubernetes.io/token' \
        --from-literal description="cluster bootstrap token" \
        --from-literal token-id=${TOKEN_PUB} \
        --from-literal token-secret=${TOKEN_SECRET} \
        --from-literal usage-bootstrap-authentication=true \
        --from-literal usage-bootstrap-signing=true
```

建立bootstrap的kubeconfig文件

```
# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/kubernetesTLS/ca.crt --embed-certs=true --server=https://192.168.88.30:6443 --kubeconfig=/etc/kubernetes/bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context kubelet-bootstrap@kubernetes --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=/etc/kubernetes/bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kube-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=/etc/kubernetes/bootstrap.kubeconfig

# 设置当前使用的上下文
kubectl config use-context kubelet-bootstrap@kubernetes --kubeconfig=/etc/kubernetes/bootstrap.kubeconfig

# 查看生成的配置文件
kubectl config view --kubeconfig=/etc/kubernetes/bootstrap.kubeconfig
```

授权kubelet可以创建csr

```
kubectl create clusterrolebinding kubeadm:kubelet-bootstrap \
        --clusterrole system:node-bootstrapper --group system:bootstrappers
```

- nodeclient: kubelet 首次创建新证书以 O=system:bootstrappers 和 CN=system:bootstrappers 形式发起的 CSR 请求
- selfnodeclient: kubelet client 更新自己的证书以 O=system:nodes 和 CN=system:nodes 形式发起的 CSR 请求

允许system:bootstrappers组的所有csr
```
cat <<EOF | kubectl apply -f -
# Approve all CSRs for the group "system:bootstrappers"
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auto-approve-csrs-for-group
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
  apiGroup: rbac.authorization.k8s.io
EOF
```

允许kubelet能够更新自己的证书
```
cat <<EOF | kubectl apply -f -
# Approve renewal CSRs for the group "system:nodes"
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auto-approve-renewals-for-nodes
subjects:
- kind: Group
  name: system:nodes
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
  apiGroup: rbac.authorization.k8s.io
EOF
```

### Kubernetes Node

在全部的节点都需要安装kubelet和kube-proxy

#### kubelet

```
mkdir -p /etc/kubernetes/manifests
```

- /var/kubernetes/kubelet
```
vim /etc/kubernetes/kubelet

# kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"
#
## The port for the info server to serve on
KUBELET_PORT="--port=10250"
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=192.168.88.38"
#
## location of the api-server
KUBELET_CONFIG="--kubeconfig=/etc/kubernetes/kubelet.kubeconfig"
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=harbor.zjhw.com/library/pause-amd64:3.1"
#
## Add your own!
KUBELET_ARGS="--cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig  --cert-dir=/etc/kubernetes/kubernetesTLS --config=/etc/kubernetes/kubelet-conf.yml"
```

- kubelet.service
```
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_ALLOW_PRIV \
            $KUBE_LOG_LEVEL \
            $KUBELET_CONFIG\
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBELET_POD_INFRA_CONTAINER \
            $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

```
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/kubernetesTLS/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.50.0.200
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
```
