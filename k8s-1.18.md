## Kubernetes 1.18.2 高可用集群安装

|  IP   | Hostname | OS |CPU | Memory | role |
| ----- | ----- | ----- | ----- | ----- | ----- |
| 192.168.88.38 | k8s-master1 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.39 | k8s-master2 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.40 | k8s-master3 | Centos7.5 | 4 | 8G | etcd， master |
| 192.168.88.41 | k8s-master4 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.42 | k8s-master5 | Centos7.5 | 4 | 8G | etcd，master |
| 192.168.88.43 | k8s-node1 | Centos7.5 | 8 | 16G | node |
| 192.168.88.44 | k8s-node1 | Centos7.5 | 8 | 16G | node |
| 192.168.88.31 | HA1 | Centos7.5 | 2 | 4G | haproxy，keepalived |
| 192.168.88.32 | HA2 | Centos7.5 | 2 | 4G | haproxy，keepalived |


VIP地址 192.168.88.30 master节点的keepalived+haproxy来选择VIP归属保持高可用

### 准备工作

- 时间同步
- 双机互信
- 关闭selinux
```
setenforce 0
sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config
```
- 关闭防火墙
```
systemctl stop firwalld
systemctl disable firewalld
systemctl mask firewalld
```
- 关闭swap
```
swapoff -a && sysctl -w vm.swappiness=0
sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab
```
- 升级系统
```
yum update
```
- 安装通用包
```
yum install -y epel-release wget git jq psmisc socat ipvsadm ipset sysstat libseccomp curl net-tools ethtool
```
- 支持ipvs
```
module=(
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
br_netfilter
  )
for kernel_module in ${module[@]};do
    /sbin/modinfo -F filename $kernel_module |& grep -qv ERROR && echo $kernel_module >> /etc/modules-load.d/ipvs.conf || :
done
systemctl enable --now systemd-modules-load.service
```
- 内核参数
```
# net
## 开启time_wait状态重用机制
net.ipv4.tcp_tw_reuse = 1
## 减少允许time_wait状态的数量，默认180000
net.ipv4.tcp_max_tw_buckets = 6000
## 减少fin_wait_2状态的时间，默认60，防止对端长时间不响应导致占用大量的socket套接字
net.ipv4.tcp_fin_timeout = 10
## 在放弃连接之前syn重试的次数
net.ipv4.tcp_syn_retries = 1
## 定义了内核在放弃连接之前所送出的syn+ack的数据，默认5，大约会花费180秒
net.ipv4.tcp_synack_retries = 1
## 防治ddos攻击，synflood
net.ipv4.tcp_syncookies = 1
## 系统最多有多少个套接字不被关联到任一个用户句柄，所谓的孤儿连接，简单防护ddos工具，内存增大这个值也应该被增大
net.ipv4.tcp_max_orphans = 3276800
## 半连接队列，对于未获得对方确认的连接请求，可以保存在这个队列，服务器网络异常中断可以排查这个参数
net.ipv4.tcp_max_syn_backlog = 262144
## 每个端口接受的数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目
net.core.netdev_max_backlog = 262144
## 表示每个套接字所允许的最大缓冲区的大小
net.core.optmem_max = 81920 
## 关闭tcp时间戳功能，tcp存在一种行为，可以缓存每个连接最新的时间戳，后续请求中如果时间戳小于缓存的时间戳，即视为无效，相应的数据包会被丢弃
net.ipv4.tcp_timestamps = 0
## 间隔多久发一次keepalive探测包，默认7200
net.ipv4.tcp_keepalive_time = 30
## 探测失败后，间隔多久后重新探测，默认75秒
net.ipv4.tcp_keepalive_intvl = 30
## 探测失败后，最多尝试几次，默认9次
net.ipv4.tcp_keepalive_probes = 3
## 根据业务尽量避免业务端口被随机使用
net.ipv4.ip_local_port_range = 12000 65000
## 默认的TCP数据接收窗口大小
net.core.rmem_default = 8388608
## 最大的TCP数据接收窗口大小
net.core.rmem_max = 16777216
## 默认发送TCP数据窗口大小
net.core.wmem_default = 8388608
## 最大的TCP发送数据窗口大小
net.core.wmem_max = 16777216
## 内存使用的下限 警戒值 上限值（内存页）
net.ipv4.tcp_mem = 94500000 915000000 927000000
## socket接收缓冲区内存使用的下限 警戒值 上限（内存页）
net.ipv4.tcp_rmem = 4096  87380   4194304
## socket发送缓冲区内存使用的下限 警戒值 上限（内存页）
net.ipv4.tcp_wmem = 4096  16384   4194304
## 启用有选择的应答,通过有选择地应答乱序接收到的报文来提高性能
net.ipv4.tcp_sack = 1
## 启用转发应答，可以进行有选择应答（SACK）从而减少拥塞情况的发生
net.ipv4.tcp_fack = 1
## 启用RFC 1323定义的window scaling，要支持超过64KB的TCP窗口，必须启用该值
net.ipv4.tcp_window_scaling = 1
## 开启反向路径过滤
net.ipv4.conf.default.rp_filter = 1
## 禁用ip源路由
net.ipv4.conf.default.accept_source_route = 0
## 开启路由转发功能
net.ipv4.ip_forward = 1
## 关闭ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

## iptables不对bridge的数据进行处理
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
# kernel
## 以字节为单位规定单一信息队列的最大值
kernel.msgmnb = 65536
## 以字节为单位规定信息队列中任意信息的最大允许的大小
kernel.msgmax = 65536
## 以字节为单位规定一次在该系统中可以使用的共享内存总量
kernel.shmall = 4294967296
## 以字节为单位内核可允许的最大共享内存 
kernel.shmmax = 68719476736
## 使用sysrq组合键是了解系统目前运行情况，为安全起见设为0关闭
kernel.sysrq = 0
## 控制core文件的文件名是否添加pid作为扩展
kernel.core_uses_pid = 1

# mem
## 优先互动性并尽量避免将进程装换出物理内存
vm.swappiness = 0
## 定义一个进程能够拥有的最多的内存区域，jvm要求高时，这个值也必须调大
vm.max_map_count = 65535
```

- 检查系统内核和模块是否适合运行docker
```
curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh > check-config.sh
bash ./check-config.sh
....
- CONFIG_USER_NS: enabled
  (RHEL7/CentOS7: User namespaces disabled; add 'user_namespace.enable=1' to boot command line)
....
```
解决:
```
# kernel 设置
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
# 写入配置文件
echo "user.max_user_namespaces=10000" >> /etc/sysctl.conf
# 重启
reboot
```
参考：https://www.123si.org/os/article/centos-7-enable-user-namespaces/

- 准备Kubernetes二进制包

```
docker pull jusene/k8s-1.18.2-bin
docker run -d --rm jusene/k8s-1.18.2-bin:latest sleep 300
docker cp 1ee:/kubernetes-1.18.2.tar.gz .
tar xf kubernetes-1.18.2.tar.gz -C /usr/local/bin
# 分发到全部的master节点 kube-apiserver kube-scheduler kube-controller-manager kubectl kubelet kube-proxy
# 分发到全部的node节点 kubelet kube-proxy
```

- 准备Kubernetes CNI二进制文件

```
wget https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz
tar xf cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin

## CNI分发到其他节点的 /opt/cni/bin 下
```

### 安装ETCD集群

- CN: Common Name, apiserver 会从证书中提取该字段作为请求的用户名 (User Name)
- O Organization, apiserver 会从证书中提取该字段作为请求用户所属的组 (Group)

准备openssl.cnf
```
[ req ]
default_bits = 2048
default_md = sha256
distinguished_name = req_distinguished_name

[req_distinguished_name]

[ v3_ca ]
basicConstraints = critical, CA:TRUE
keyUsage = critical, digitalSignature, keyEncipherment, keyCertSign

[ v3_req_server ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth

[ v3_req_client ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth

[ v3_req_apiserver ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names_cluster

[ v3_req_etcd ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
subjectAltName = @alt_names_etcd

[ alt_names_cluster ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
DNS.6 = localhost
IP.1 = 10.50.0.1  # ClusterServiceIP地址
IP.2 = 127.0.0.1
IP.3 = 192.168.88.30  # VIP地址
IP.4 = 10.50.0.200  # kube DNS地址

[ alt_names_etcd ]
DNS.1 = localhost
IP.1 = 127.0.0.1
IP.2 = 192.168.88.38
IP.3 = 192.168.88.39
IP.4 = 192.168.88.40
IP.5 = 192.168.88.41
IP.6 = 192.168.88.42
```

#### 生成证书

- ca证书
```
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj "/CN=etcd-ca" -extensions v3_ca -out ca.crt -days 36500
```

- etcd证书
```
openssl genrsa -out etcd.key 2048
openssl req -new -key etcd.key -subj "/CN=etcd/O=system:masters" -out etcd.csr
openssl x509 -in etcd.csr -req -CA ca.crt -CAkey ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd.crt -days 36500
```

- 准备证书
```
mkdir /etc/etcd/etcdSSL
cp ca.crt etcd.key etcd.pem /etc/etcd/etcdSSL/
chown -R etcd /etc/etcd/etcdSSL
# 同样分发到全部etcd节点
```

- 安装etcd
```
yum install -y etcd
```

- 准备配置文件
```
vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/usr/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/etcd/etcdSSL/etcd.crt \
  --key-file=/etc/etcd/etcdSSL/etcd.key \
  --peer-cert-file=/etc/etcd/etcdSSL/etcd.crt \
  --peer-key-file=/etc/etcd/etcdSSL/etcd.key \
  --trusted-ca-file=/etc/etcd/etcdSSL/ca.crt \
  --peer-trusted-ca-file=/etc/etcd/etcdSSL/ca.crt \
  --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
  --listen-peer-urls=${ETCD_LISTEN_PEER_URLS} \
  --listen-client-urls=${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
  --advertise-client-urls=${ETCD_ADVERTISE_CLIENT_URLS} \
  --initial-cluster-token=${ETCD_INITIAL_CLUSTER_TOKEN} \
  --initial-cluster=${ETCD_CLUSTER}\
  --initial-cluster-state new \
  --data-dir=${ETCD_DATA_DIR}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

- etcd.conf
```
ETCD_NAME=etcd0
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.88.38:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.88.38:2379"

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.88.38:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.88.38:2379"
ETCD_CLUSTER="etcd0=https://192.168.88.38:2380,etcd1=https://192.168.88.39:2380,etcd2=https://192.168.88.40:2380,etcd3=https://192.168.88.41:2380,etcd4=https://192.168.88.42:2380"

# 其他节点安装配置相应修改
```

5个etcd节点，需要同时启动3个节点整个集群才可以使用

```
etcdctl --cert-file=/etc/etcd/etcdSSL/etcd.crt --key-file=/etc/etcd/etcdSSL/etcd.key --ca-file=/etc/etcd/etcdSSL/ca.crt cluster-health
member 3f606110d03e3132 is healthy: got healthy result from https://192.168.88.39:2379
member 3f8913ef4acb01e6 is healthy: got healthy result from https://192.168.88.41:2379
member 72cbf487f90ef57b is healthy: got healthy result from https://192.168.88.42:2379
member d58a14e5945489fa is healthy: got healthy result from https://192.168.88.38:2379
member dd01c3c17c734d25 is healthy: got healthy result from https://192.168.88.40:2379
cluster is healthy
```

### 安装Kubernetes Master

#### 部署HA(haproxy+keepalived)

```
yum install -y haproxy keepalived
```

- haproxy
```
vim /etc/haproxy/haproxy.cfg

global
  maxconn  5000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

listen stats
  bind    *:8006
  mode    http
  stats   enable
  stats   hide-version
  stats   uri       /stats
  stats   refresh   30s
  stats   realm     Haproxy\ Statistics
  stats   auth      admin:admin

frontend k8s-api
  bind 0.0.0.0:6443
  bind 127.0.0.1:6443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-api

backend k8s-api
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 500 maxqueue 256 weight 100
  server api1 192.168.88.38:6443 check
  server api2 192.168.88.39:6443 check
  server api3 192.168.88.40:6443 check
  server api4 192.168.88.41:6443 check
  server api5 192.168.88.42:6443 check
```

- keepalived

```
global_defs {
   vrrp_mcast_group4 224.0.100.20
}


vrrp_script chk_haproxy {
      script "kill -0 `pidof haproxy`"
      interval 2
      weight -10
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens3
    virtual_router_id 100
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass dd@2019
    }
    virtual_ipaddress {
        192.168.88.30/24 dev ens3 label ens3:0
    }
    track_script {
        chk_haproxy
    }
}
```

在两台ha服务器上进行相同的配置

```
systemctl enable haproxy --now
systemctl enable keepalived --now
```

#### 准备证书

